<html><head lang="ja"><link rel="icon" type="image/png" href="favicon.png"/><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/><meta property="og:type" content="article"/><meta property="og:image" content="https://hinaser.github.io/Machine-Learning/preview.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://hinaser.github.io/Machine-Learning/preview.png"/><meta name="twitter:image:width" content="1200"/><meta name="twitter:image:height" content="630"/><meta name="author" content="Hinase"/><meta name="keywords" content="TensorFlow, Machine Learning, Math, Mathematics"/><meta property="og:title" content="機械学習のための数学"/><meta property="og:description" content="機械学習を理解するために必要な数学を解説します。"/><meta property="og:url" content="https://hinaser.github.io/Machine-Learning/math-for-ml.html"/><meta name="twitter:title" content="機械学習のための数学"/><meta name="twitter:description" content="機械学習を理解するために必要な数学を解説します。"/><meta name="twitter:url" content="https://hinaser.github.io/Machine-Learning/math-for-ml.html"/><title>機械学習のための数学</title><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"/><link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css"/><script defer="" src="https://code.getmdl.io/1.3.0/material.min.js"></script><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"/><link href="https://fonts.googleapis.com/earlyaccess/notosansjapanese.css" rel="stylesheet"/><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link rel="stylesheet" href="css/lib.css" type="text/css" data-turbolinks-track="reload"/><link rel="stylesheet" href="css/main.css?ac36d58eca" type="text/css" data-turbolinks-track="reload"/><script id="st_insights_js" type="text/javascript" src="https://ws.sharethis.com/button/buttons.js?publisher=8fde68e6-f234-4808-b7ab-c498ac41fe48"></script><script type="text/javascript">stLight.options({
    publisher: "8fde68e6-f234-4808-b7ab-c498ac41fe48",
    doNotHash: true,
    doNotCopy: true,
    hashAddressBar: false
});</script><script src="js/lib.js?6099111fed" data-turbolinks-track="reload"></script></head><body><div class="dpln-layout top-header-left-nav-layout fixed-top"><header class="dpln-layout-header"><i class="material-icons" id="toggle-nav">view_headline</i><span class="site-title">機械学習、ディープラーニング</span></header><nav class="dpln-layout-nav"><figure class="collapsible" data-page="learntensorflowplayground"><figcaption><span class="menu-title">TensorFlow Playgroundの仕組み</span><button class="mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">keyboard_arrow_down</i></button></figcaption><ul><li><a href="index.html#preface-part">はじめに</a></li><li><a href="index.html#reference-part">参考文献</a></li><li><a href="index.html#summary-part">Playgroundの仕組み</a></li><li><a href="index.html#learningsettings-part">ニューラルネットワークについて</a></li><li><a href="index.html#about-learning-in-nn-part">ニューラルネットワークの学習に関する設定</a></li><li><a href="index.html#learningrate-part">学習率</a></li><li><a href="index.html#activator-part">活性化関数</a></li><li><a href="index.html#regularization-part">正則化、正則化項</a></li><li><a href="index.html#problemtype-part">統計モデルの種類</a></li><li><a href="index.html#inputdata-part">データに関する設定</a></li><li><a href="index.html#dataselection-part">どのデータセットを使うか</a></li><li><a href="index.html#ratio-of-training-data-part">トレーニングデータの割合</a></li><li><a href="index.html#noise-part">ノイズ</a></li><li><a href="index.html#batchsize-part">バッチサイズ</a></li><li><a href="index.html#feature-part">入力するデータの特徴と隠し層</a></li><li><a href="index.html#backpropagation-part">バックプロパゲーション(誤差逆伝搬法)</a></li></ul></figure><figure class="collapsible" data-page="deeplearning-by-tensorflow"><figcaption><span class="menu-title">TensorFlowでディープラーニング</span><button class="mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">keyboard_arrow_down</i></button></figcaption><ul><li><a href="deeplearning-by-tensorflow.html">TensorFlowのインストール</a></li><li><a href="deeplearning-by-tensorflow-with-gpu.html">TensorFlowのインストール(GPU利用編)</a></li></ul></figure><figure class="collapsible" data-page="math-for-ml"><figcaption><span class="menu-title">機械学習のための数学</span><button class="mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">keyboard_arrow_down</i></button></figcaption><div class="dpln-expand"></div><ul><li><a href="math-for-ml.html#preface">はじめに</a></li><li><a href="math-for-ml.html#random-variable">確率変数</a></li><li><a href="math-for-ml.html#probability-distribution">確率分布</a></li><li><a href="math-for-ml.html#pmf">離散確率分布</a></li><li><a href="math-for-ml.html#pdf">連続確率分布</a></li><li><a href="math-for-ml.html#expectation">期待値</a></li><li><a href="math-for-ml.html#self-information">自己情報量</a></li><li><a href="math-for-ml.html#entropy">エントロピー</a></li></ul></figure><figure class="collapsible" data-page="[&quot;profile&quot;, &quot;reference&quot;, &quot;useful-tools-and-services&quot;]"><figcaption><span class="menu-title">その他</span><button class="mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">keyboard_arrow_down</i></button></figcaption><ul><li><a href="profile.html">著者プロフィール</a></li><li><a href="reference.html">参考サイト集</a></li><li><a href="useful-tools-and-services.html">お世話になっているツール/サービス</a></li></ul></figure></nav><main class="dpln-layout-main"><article class="main-article" data-page="math-for-ml"><a class="in-page-anchor" id="preface"></a><h1>機械学習のための数学</h1><div class="paragraph"><p>機械学習のより深い理解に必要な数学の基礎についてまとめます。<br/><small>2017/3/11現在加筆中です。内容は適宜追補します。</small></p><p>このページで取り扱う記号の記法については<a href="http://www.deeplearningbook.org/contents/notation.html" target="_blank">Goodfellow et al.(2016)</a>を踏襲します。</p></div><div class="paragraph"><a class="in-page-anchor" id="random-variable"></a><h4>Random variable - 確率変数</h4><h6>特徴</h6><ul class="features"><li>確率変数と書くと大仰で難解な概念に思えるが、英語名のRandom variableで覚えると理解しやすい。ただの取りうる値がランダムな変数。
Stocastic variableともいう。Stocasticは確率論的なという意味。</li><li>難しく書くと確率変数$X$は標本空間$\Omega$から可測空間$E$への可測関数。</li><li>身近な例ではコイン投げにおいて、標本空間$\Omega$は'表'と'裏'の2つの元を持つ集合である。
'表'と'裏'のラベルのままだと数学的に扱いにくいので、通常は'表'=1, '裏'=0と標本空間の元に数を割り当てる。
この時標本空間に対応づけられている$\{0,1\}$が確率変数$X$の取りうる値の集合となる。</li><li>確率変数は離散値をとる場合と連続的な値をとる場合の2種類ある。</li><li>確率変数が離散的であるとは$X$の個数が有限または可算無限であることをいう。</li><li>確率変数が連続的であるとは$X$の個数が非可算であることをいう。</li><li>確率変数$X$の発生しやすさの確率分布関数$P(X)$が合わせて定義される。確率変数$X$の値そのものは確率(起こりやすさ)とは関係がない。</li></ul><h6>厳密な理解に必要な知識</h6><ul class="prerequisite"><li><a href="https://en.wikipedia.org/wiki/Measure_(mathematics)" target="_blank">Measure Theory - 測度論</a></li><li><a href="https://www.encyclopediaofmath.org/index.php/Measurable_space" target="_blank">Measuable Space - 可測空間</a></li><li><a href="https://en.wikipedia.org/wiki/Measurable_function" target="_blank">Measure Function - 可測関数</a></li><li><a href="https://en.wikipedia.org/wiki/Probability_space" target="_blank">Probability Space - 確率空間</a></li></ul></div><div class="paragraph"><a class="in-page-anchor" id="probability-distribution"></a><h4>Probabiity distribution function - 確率分布関数</h4><h6>特徴</h6><ul class="features"><li>確率変数$X$の起こりやすさを表した関数。確率変数が離散的であるか連続的であるかによって確率分布関数の性質は異なる。
前者の場合の確率分布関数をPMF(Probability Mass Function, 確率質量関数)と呼び、後者をPDF(Probability Density Function、 確率密度関数)と呼ぶ。</li><li>PMFを考えるとき、一般に確率分布関数は大文字の$P$で表記される。</li><li>PDFを考えるとき、一般に確率分布関数は小文字の$p$で表記される。</li><li>$X$が確率分布関数$P$にしたがって分布するとき、
$$ X \sim P$$
と書く。</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="pmf"></a><h4>Discrete Probabiity distribution function - 離散確率分布関数</h4><h6>特徴</h6><ul class="features"><li>$$ 0 \le P(X=x) \le 1 $$</li><li>$$ \sum_{x \in X} P(x) = 1 $$</li><li>$P(X=x)$は事象$x$の起こりえる確率をそのまま表す。</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="pdf"></a><h4>Continuous Probabiity distribution function - 連続確率分布関数</h4><h6>特徴</h6><ul class="features"><li>$$ 0 \le p(X=x) $$</li><li>$$ \int_{x \in X} p(x) dx = 1 $$</li><li>$P(X=x)$は離散確率分布と異なり事象$x$の起こりえる確率を直接表さない。
非可算無限個の元を持つ確率変数$X$に対して無限分の1である$X=x$をピンポイントで
拾い上げる確率は限りなくゼロに近いため。
通常は$X$上の連続区間$I \in [a,b]$を考え、区間$I$で表せられる事象が発生する確率を
$$ \int_{a}^{b} p(x) dx $$で表す。</li><li>連続確率分布の例として一様分布(Uniform Distribution)がある。これは確率分布関数が任意の閉区間$I \in [a,b]$において、
$$ p(x) = u(x;a,b) = \frac{1}{b-a}$$
で表される確率密度関数である。
区間の長さのみによって確率が決まる非常にシンプルな分布関数である。</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="expectation"></a><h4>Expectation - 期待値</h4><h6>特徴</h6><ul class="features"><li>離散確率分布の場合:
$$ \mathbb{E}_{x \sim P}[f(x)] = \sum_{x} P(x) f(x) $$</li><li>連続確率分布の場合:
$$ \mathbb{E}_{x \sim P}[f(x)] = \int_{x} p(x) f(x) dx $$</li><li>線形性:
$$ \mathbb{E}_{x \sim P}[\alpha f(x) + \beta g(x)] = \alpha \mathbb{E}[f(x)] + \beta \mathbb{E}[g(X)] $$</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="self-information"></a><h4>Self Information - 自己情報量</h4><h6>特徴</h6><ul class="features"><li>surprisalとも言う。情報の量を定義するために用いられる概念。</li><li>$P(x)=1$、つまりわかりきっている情報は情報量ゼロとなる。</li><li>まれな情報ほど自己情報量$I(x)$が大きい。</li><li>定義:
$$I(x) = - \log P(x)$$</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="entropy"></a><h4>Entropy - エントロピー</h4><h6>特徴</h6><ul class="features"><li>平均情報量とも言う。</li><li>自己情報量ではある単一の事象$x$における情報量しかわからなかったが、エントロピーの概念を用いれば、
標本空間全体の情報量を定量することができる。</li><li>定義:
$$ H(X) = E_{X \sim P}[I(X)] = - \sum_{X \sim P} P(X) \ln{P(X)}$$</li><li>$X \sim P$とすると、エントロピーは$$H(P)$$とも表記される。</li><li>起こりうる事象の数を固定すると、分布関数の選び方によってエントロピーの大小が変わる。
a個の事象を考えるとき、それぞれの事象の確率が等しく$1/a$であるような一様分布はこの事象のエントロピーを最大にする分布の選び方である。
これは事象の不確定さが最大ということを意味する。
不確定さが小さくなるほどエントロピーも小さくなる。</li></ul></div><div class="paragraph"><a class="in-page-anchor" id="cross-entropy"></a><h4>Kullback-Leibler divergence - カルバック・ライブラー情報量</h4><p></p></div><div class="paragraph"><a class="in-page-anchor" id="cross-entropy"></a><h4>Cross Entropy - クロスエントロピー</h4><p></p></div><div class="paragraph"><a class="in-page-anchor" id="logits"></a><h4>Logits - ロジット</h4><p></p></div><div class="paragraph"><a class="in-page-anchor" id="softmax"></a><h4>Softmax - ソフトマックス</h4><p></p></div></article><script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$']],
processEscapes: true
},
CommonHTML: {matchFontHeight: false },
displayAlign: "left",
displayIndent: "2em"
});

</script></main></div><script src="js/main.js?0c38373d66" data-turbolinks-track="reload"></script><script src="js/raw/analytics.js" data-turbolinks-track="reload"></script></body></html>